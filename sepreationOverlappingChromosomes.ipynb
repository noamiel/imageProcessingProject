{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noamiel/imageProcessingProject/blob/main/ImageProcessingProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iHlAWQ0p-jil"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from matplotlib import *\n",
        "import sys\n",
        "import pylab as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sudo pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO8wwPerrW-7",
        "outputId": "e6f89da4-e7a5-4052-d1f5-3e7857485c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omnipose\n",
            "  Downloading omnipose-0.2.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting edt\n",
            "  Downloading edt-2.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from omnipose) (1.7.3)\n",
            "Collecting cellpose\n",
            "  Downloading cellpose-2.1.0-py3-none-any.whl (169 kB)\n",
            "\u001b[K     |████████████████████████████████| 169 kB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from omnipose) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from omnipose) (0.56.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from omnipose) (0.18.3)\n",
            "Collecting ncolor\n",
            "  Downloading ncolor-1.1.5.tar.gz (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (5.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (4.64.1)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (2021.11.2)\n",
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2021.11.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.0 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (0.39.1)\n",
            "Collecting fastremap\n",
            "  Downloading fastremap-1.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (4.6.0.66)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from cellpose->omnipose) (1.12.1+cu113)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->omnipose) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->omnipose) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->cellpose->omnipose) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->omnipose) (3.8.1)\n",
            "Collecting ncolor\n",
            "  Downloading ncolor-1.1.2-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->omnipose) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->omnipose) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->omnipose) (3.2.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->omnipose) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->omnipose) (2.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->omnipose) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->omnipose) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->omnipose) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->omnipose) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->omnipose) (1.15.0)\n",
            "Installing collected packages: imagecodecs, fastremap, ncolor, edt, cellpose, omnipose\n",
            "Successfully installed cellpose-2.1.0 edt-2.3.0 fastremap-1.13.3 imagecodecs-2021.11.20 ncolor-1.1.2 omnipose-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install omnipose\n",
        "import omnipose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mv3-QVvplqEo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dNjO4CKY3Y5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecbc672-62db-45ce-9aa6-797d0c68c86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/Colab Notebooks/project\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/My Drive/Colab Notebooks/project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9BNDXzRPKgjz"
      },
      "outputs": [],
      "source": [
        "def plotSamplesOneHots(labels_of_samples, output_file=False):\n",
        "    '''\n",
        "    labels_of_samples of shape (num_samples, x, y, num_onehots)\n",
        "    '''\n",
        "    if len(labels_of_samples.shape) != 4:\n",
        "        print(\"Incorrect input size - should be (num_samples, x, y, num_onehots)\")\n",
        "    num_samples = labels_of_samples.shape[0]\n",
        "    num_onehots = labels_of_samples.shape[1]\n",
        "    figure_size = (4*num_onehots, 4*num_samples)\n",
        "    fig, ax = plt.subplots(num_samples, num_onehots, sharex=True, sharey=True, figsize=figure_size)\n",
        "    for i in range(num_samples):\n",
        "        for j in range(num_onehots):\n",
        "            ax[i, j].imshow(labels_of_samples[i,j,...], aspect=\"auto\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    if output_file == True:\n",
        "        fig.savefig(output_file)\n",
        "        \n",
        "def makeXbyY(data, X, Y):\n",
        "    '''\n",
        "    Crop data to size X by Y\n",
        "    '''\n",
        "    if len(data.shape) < 3:\n",
        "        print('Input should be of size (num_samples, x, y,...)')\n",
        "    data_x_start = int((data.shape[1]-X)/2)\n",
        "    data_y_start = int((data.shape[1]-Y)/2)\n",
        "    arrayXbyY = data[:, (data_x_start):(data_x_start + X), (data_y_start):(data_y_start + Y),...]\n",
        "    return arrayXbyY\n",
        "\n",
        "def findNearestNeighbourLabel(array):\n",
        "    center = int(array.shape[0]/2)\n",
        "    labels_count = np.zeros(5)\n",
        "    for x in range(array.shape[0]):\n",
        "        for y in range(array.shape[1]):\n",
        "            if (x != center) or (y != center):\n",
        "                temp_label = array[x, y]\n",
        "                labels_count[temp_label] += 1\n",
        "    return labels_count.argmax()\n",
        "\n",
        "def cleanLabelNearestNeighbour(label,num_of_classes):\n",
        "    '''\n",
        "    Corrects incorrect labels in a single image based on a threshold on the number of \n",
        "    nearest neighbours with the same label\n",
        "    '''\n",
        "    x_length = label.shape[0]\n",
        "    y_length = label.shape[1]\n",
        "    # num_of_classes = 4\n",
        "    cleaned_labels = np.zeros((x_length, y_length, 4))\n",
        "    for x in range(1,x_length-1):\n",
        "        for y in range(1, y_length-1):\n",
        "            temp_label = label[x,y]\n",
        "            if temp_label >3: # if labeled as 4 or above\n",
        "                temp_label = findNearestNeighbourLabel(label[(x-1):(x+2), (y-1):(y+2)])\n",
        "                cleaned_labels[x, y, temp_label] = 1\n",
        "            elif temp_label > 0:\n",
        "                num_labels_in_3x3 = len(np.where(label[(x-1):(x+2), (y-1):(y+2)]==temp_label)[0])\n",
        "                if num_labels_in_3x3 > 3:\n",
        "                    cleaned_labels[x, y, temp_label] = 1\n",
        "                else:\n",
        "                    temp_label = findNearestNeighbourLabel(label[(x-1):(x+2), (y-1):(y+2)])\n",
        "                    cleaned_labels[x, y, temp_label] = 1\n",
        "        non_zero_array = cleaned_labels[..., 1:].sum(axis=2).astype('bool')\n",
        "        cleaned_labels[..., 0] = np.ones((x_length, y_length), dtype='bool')^non_zero_array\n",
        "    return cleaned_labels\n",
        "\n",
        "def cleanLabelNearestNeighbour_alllabels(labels):    \n",
        "    '''\n",
        "    Cleans incorrect labels\n",
        "    '''\n",
        "    num_labels = labels.shape[0] # count of data set\n",
        "    num_of_classes = 4\n",
        "    cleaned_dim = list(labels.shape) #[13434, 94, 93]\n",
        "    cleaned_dim.append(num_of_classes) # [13434, 94, 93,4]\n",
        "    cleaned_labels = np.zeros(cleaned_dim)\n",
        "    for image_i in range(num_labels):\n",
        "        # print('Preprocessing image %d of %d' % (image_i, num_labels))\n",
        "        cleaned_labels[image_i,...] = cleanLabelNearestNeighbour(labels[image_i, ...],num_of_classes)\n",
        "    return cleaned_labels\n",
        "\n",
        "def meanIOU_per_image(y_pred, y_true):\n",
        "    '''\n",
        "    Calculate the IOU, averaged across images\n",
        "    '''\n",
        "    if len(y_pred.shape) < 3 or (y_pred.shape[2]<4):\n",
        "        print('Wrong dimensions: one hot encoding expected')\n",
        "        return\n",
        "    y_pred = y_pred.astype('bool')\n",
        "    y_true = y_true.astype('bool')\n",
        "    IUs = []\n",
        "    for layer in range(y_true.shape[1]):\n",
        "        intersection = y_pred[:,layer,...] & y_true[:,layer,...]\n",
        "        union = y_pred[:,layer,...] | y_true[:,layer,...]\n",
        "        if union.sum() == 0:\n",
        "            IUs.append(1)\n",
        "        else:\n",
        "            IUs.append(intersection.sum()/union.sum())\n",
        "    return sum(IUs)/len(IUs)\n",
        "\n",
        "def meanIOU(y_pred, y_true):\n",
        "    '''\n",
        "    Calculate the mean IOU, with the mean taken over classes\n",
        "    '''\n",
        "    if len(y_pred.shape) < 4 or (y_pred.shape[1]<4):\n",
        "        print('Wrong dimensions: one hot encoding expected')\n",
        "        return\n",
        "    y_pred = y_pred.astype('bool')\n",
        "    y_true = y_true.astype('bool')\n",
        "    IUs = []\n",
        "    for layer in range(y_true.shape[1]):\n",
        "        intersection = y_pred[:,layer,...] & y_true[:,layer,...]\n",
        "        union = y_pred[:,layer,...] | y_true[:,layer,...]\n",
        "        if union.sum() == 0:\n",
        "            IUs.append(1)\n",
        "        else:\n",
        "            IUs.append(intersection.sum()/union.sum())\n",
        "    return sum(IUs)/len(IUs)\n",
        "\t\n",
        "def IOU(y_pred, y_true):\n",
        "    '''\n",
        "    Calculate the IOU for each class seperately\n",
        "    '''\n",
        "    if len(y_pred.shape) < 4 or (y_pred.shape[1]<4):\n",
        "        print('Wrong dimensions: one hot encoding expected')\n",
        "        return\n",
        "    y_pred = y_pred.astype('bool')\n",
        "    y_true = y_true.astype('bool')\n",
        "    #print(y_pred)\n",
        "    #print(y_true)\n",
        "    IUs = []\n",
        "    for layer in range(y_true.shape[1]):\n",
        "        intersection = y_pred[:,layer,...] & y_true[:,layer,...]\n",
        "        union = y_pred[:,layer,...] | y_true[:,layer,...]\n",
        "        #print(intersection.sum(), union.sum())\n",
        "        if union.sum() == 0:\n",
        "            IUs.append(1)\n",
        "        else:\n",
        "            IUs.append(intersection.sum()/union.sum())\n",
        "    return IUs\n",
        "\n",
        "# One-hot encoding\n",
        "def oneHotEncode(initial_array):\n",
        "    '''\n",
        "    One hot encode the labels\n",
        "    '''\n",
        "    allowed_max_class_num = 3\n",
        "    output_shape = list(initial_array.shape)\n",
        "    output_shape[-1] = initial_array.max()\n",
        "    output_array_dims = list(initial_array.shape)\n",
        "    output_array_dims.append(4)\n",
        "    output_array = np.zeros(output_array_dims)\n",
        "    for image_i in range(0, initial_array.shape[0]):\n",
        "        for class_num in range(0, allowed_max_class_num):\n",
        "            for x in range(0, initial_array.shape[1]):\n",
        "                for y in range(0, initial_array.shape[2]):\n",
        "                    if initial_array[image_i, x, y] == class_num:\n",
        "                        output_array[image_i, x, y, class_num] = 1\n",
        "\n",
        "        class_num = allowed_max_class_num\n",
        "        for x in range(0, initial_array.shape[1]):\n",
        "            for y in range(0, initial_array.shape[2]):\n",
        "                if initial_array[image_i, x, y] >= allowed_max_class_num:\n",
        "                    output_array[image_i, x, y, class_num] = 1\n",
        "    return output_array\n",
        "\n",
        "# Global Accuracy\n",
        "def globalAccuracy(y_pred, y_true):\n",
        "    # Calculate the global accuracy (ie. percent of pixels correctly labelled)\n",
        "    \n",
        "    y_pred = y_pred.astype('bool')\n",
        "    y_true = y_true.astype('bool')\n",
        "\n",
        "    correct = y_pred & y_true\n",
        "    num_correct = correct.sum()\n",
        "    num_total = 1\n",
        "    shape_dim=list(y_true.shape)\n",
        "    shape_dim.remove(4)\n",
        "    shape_dim\n",
        "    for dim in shape_dim:\n",
        "        # print(dim)\n",
        "        num_total = num_total*dim\n",
        "    return num_correct/num_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yAgy7QpsLekh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abafdc1-933f-437f-cfc1-ec0347d6d49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-18 04:21:17--  https://github.com/jeanpat/DeepFISH/raw/master/dataset/LowRes_13434_overlapping_pairs.h5\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/jeanpat/DeepFISH/master/dataset/LowRes_13434_overlapping_pairs.h5 [following]\n",
            "--2022-09-18 04:21:17--  https://raw.githubusercontent.com/jeanpat/DeepFISH/master/dataset/LowRes_13434_overlapping_pairs.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24203163 (23M) [application/octet-stream]\n",
            "Saving to: ‘LowRes_13434_overlapping_pairs.h5’\n",
            "\n",
            "LowRes_13434_overla 100%[===================>]  23.08M  61.1MB/s    in 0.4s    \n",
            "\n",
            "2022-09-18 04:21:18 (61.1 MB/s) - ‘LowRes_13434_overlapping_pairs.h5’ saved [24203163/24203163]\n",
            "\n",
            "Shape of Images: (13434, 94, 93) \n",
            "Shape of Labels: (13434, 94, 93)\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://github.com/jeanpat/DeepFISH/raw/master/dataset/LowRes_13434_overlapping_pairs.h5'\n",
        "!mv 'LowRes_13434_overlapping_pairs.h5' dataset/\n",
        "\n",
        "file_path = 'dataset/LowRes_13434_overlapping_pairs.h5'\n",
        "h5f = h5py.File(file_path,'r')\n",
        "xdata = h5f['dataset_1'][...,0]\n",
        "labels = h5f['dataset_1'][...,1]\n",
        "h5f.close()\n",
        "print(f'Shape of Images: {xdata.shape} \\nShape of Labels: {labels.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_3SBcS8aZ4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b6fdb1-42fc-4056-a5a1-9a75c98e1e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  -4    2]\n",
            " [  -3    4]\n",
            " [  -2   12]\n",
            " [  -1   21]\n",
            " [   0 7951]\n",
            " [   1   19]\n",
            " [   2    7]\n",
            " [   3    4]\n",
            " [   4    8]\n",
            " [   5    1]\n",
            " [   6    7]\n",
            " [   7    9]\n",
            " [   8    2]\n",
            " [   9    6]\n",
            " [  10    3]\n",
            " [  11    4]\n",
            " [  13    9]\n",
            " [  14    9]\n",
            " [  15    9]\n",
            " [  16    2]\n",
            " [  17    4]\n",
            " [  18    4]\n",
            " [  19   11]\n",
            " [  20    1]\n",
            " [  21    3]\n",
            " [  22    6]\n",
            " [  23    9]\n",
            " [  24    7]\n",
            " [  25   10]\n",
            " [  26    4]\n",
            " [  27    6]\n",
            " [  28    5]\n",
            " [  29    3]\n",
            " [  30   10]\n",
            " [  31    8]\n",
            " [  32    4]\n",
            " [  33    2]\n",
            " [  34    4]\n",
            " [  35    6]\n",
            " [  36    4]\n",
            " [  37   13]\n",
            " [  38    8]\n",
            " [  39    6]\n",
            " [  40    5]\n",
            " [  41    4]\n",
            " [  42    2]\n",
            " [  43    9]\n",
            " [  44   11]\n",
            " [  45    5]\n",
            " [  46    3]\n",
            " [  47    5]\n",
            " [  48    5]\n",
            " [  49    5]\n",
            " [  50    5]\n",
            " [  51    8]\n",
            " [  52    6]\n",
            " [  53   10]\n",
            " [  54    9]\n",
            " [  55    7]\n",
            " [  56    8]\n",
            " [  57   11]\n",
            " [  58    6]\n",
            " [  59    4]\n",
            " [  60   10]\n",
            " [  61    9]\n",
            " [  62    8]\n",
            " [  63    9]\n",
            " [  64    9]\n",
            " [  65   11]\n",
            " [  66    9]\n",
            " [  67    6]\n",
            " [  68   12]\n",
            " [  69   10]\n",
            " [  70    4]\n",
            " [  71   13]\n",
            " [  72   11]\n",
            " [  73   11]\n",
            " [  74    6]\n",
            " [  75    9]\n",
            " [  76    3]\n",
            " [  77    6]\n",
            " [  78    3]\n",
            " [  79    5]\n",
            " [  80    4]\n",
            " [  81    5]\n",
            " [  82    6]\n",
            " [  83    4]\n",
            " [  84    8]\n",
            " [  85    7]\n",
            " [  86    8]\n",
            " [  87    5]\n",
            " [  88    6]\n",
            " [  89    2]\n",
            " [  90    4]\n",
            " [  91    4]\n",
            " [  92    7]\n",
            " [  93    5]\n",
            " [  94    5]\n",
            " [  95    3]\n",
            " [  96    5]\n",
            " [  97    5]\n",
            " [  98    3]\n",
            " [  99    3]\n",
            " [ 100    5]\n",
            " [ 101    2]\n",
            " [ 102    2]\n",
            " [ 103    8]\n",
            " [ 104    2]\n",
            " [ 105    6]\n",
            " [ 106    3]\n",
            " [ 107    2]\n",
            " [ 108    3]\n",
            " [ 109    2]\n",
            " [ 110    1]\n",
            " [ 111    3]\n",
            " [ 112    2]\n",
            " [ 113    4]\n",
            " [ 114    2]\n",
            " [ 115    3]\n",
            " [ 116    5]\n",
            " [ 118    2]\n",
            " [ 119    3]\n",
            " [ 120    5]\n",
            " [ 121    5]\n",
            " [ 122    7]\n",
            " [ 123    3]\n",
            " [ 125    3]\n",
            " [ 126    6]\n",
            " [ 127    1]\n",
            " [ 128    2]\n",
            " [ 129    3]\n",
            " [ 130    2]\n",
            " [ 131    5]\n",
            " [ 132    1]\n",
            " [ 133    2]\n",
            " [ 134    2]\n",
            " [ 135    3]\n",
            " [ 138    2]\n",
            " [ 140    1]\n",
            " [ 142    1]\n",
            " [ 143    2]\n",
            " [ 144    1]\n",
            " [ 146    1]\n",
            " [ 147    1]\n",
            " [ 149    1]\n",
            " [ 157    1]\n",
            " [ 159    1]\n",
            " [ 180    1]\n",
            " [ 209    1]]\n",
            "[[   0 8028]\n",
            " [   1  181]\n",
            " [   2  433]\n",
            " [   3  100]]\n"
          ]
        }
      ],
      "source": [
        "# First overlapped chromosome image\n",
        "# This is grayscale image with shape (94, 93)\n",
        "xdata[0].shape\n",
        "\n",
        "# It is observed that some intensities of pixels are negative. I am not able to figure out why?\n",
        "np.unique(xdata[0])\n",
        "\n",
        "# Though negative intensities quantity is not much.\n",
        "\n",
        "unique, counts = np.unique(xdata[0], return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "\n",
        "# Image\n",
        "\n",
        "fig = pl.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(xdata[0], aspect=\"auto\")\n",
        "\n",
        "# Label of the first overlapped chromosome image\n",
        "# Lets see the unique intensity values of pixels\n",
        "#np.unique(labels[0])\n",
        "unique, counts = np.unique(labels[0], return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)\n",
        "\n",
        "# This clearly indicates the masks for the four classes\n",
        "# Image:\n",
        "fig = pl.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(labels[0], aspect=\"auto\")\n",
        "\n",
        "# But we can observe that there are different coulors at the boundaries. So cleaning is requried.\n",
        "\n",
        "# Clean the Labels\n",
        "labels_cleaned = cleanLabelNearestNeighbour_alllabels(labels)\n",
        "print(f'Shape of Images: {xdata.shape} \\nShape of Labels: {labels_cleaned.shape}')\n",
        "\n",
        "# Reshape image to height = width = 88\n",
        "xdata_equal = makeXbyY(xdata, 88, 88)\n",
        "labels_equal = makeXbyY(labels_cleaned, 88, 88)\n",
        "\n",
        "print(f'Shape of Images: {xdata_equal.shape} \\nShape of Labels: {labels_equal.shape}')\n",
        "\n",
        "# Reshape Data and Labels\n",
        "# Add one 1 empty channel for gray scale Images (13434, 88, 88) to (13434, 1, 88, 88)\n",
        "# Reshape labels from (13434, 88, 88, 4) to (13434, 4, 88, 88)\n",
        "xdata1 = np.expand_dims(xdata_equal, axis=1)\n",
        "labels1 = np.transpose(labels_equal, (0, 3, 1, 2))\n",
        "print(f'Shape of Images: {xdata1.shape} \\nShape of Labels: {labels1.shape}')\n",
        "\n",
        "# Lets see first two images and its final labels\n",
        "fig = pl.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(xdata1[0][0], aspect=\"auto\")\n",
        "\n",
        "fig = pl.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(xdata1[1][0], aspect=\"auto\")\n",
        "\n",
        "plotSamplesOneHots(labels1[0:2])\n",
        "\n",
        "# Save the procossed data\n",
        "np.save('dataset/xdata_88x88', xdata1)\n",
        "np.save('dataset/ydata_88x88_0123_onehot', labels1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMzP0J88bEgZ"
      },
      "outputs": [],
      "source": [
        "# Load the processed data\n",
        "xdata_loaded = np.load('dataset/xdata_88x88.npy')\n",
        "labels_loaded = np.load('dataset/ydata_88x88_0123_onehot.npy')\n",
        "\n",
        "print(f'Shape of Images: {xdata_loaded.shape} \\nShape of Labels: {labels_loaded.shape}')\n",
        "\n",
        "# Dataset Conversion in Pytorch format\n",
        "\n",
        "class ChromosomeDataset(Dataset):\n",
        "  def __init__(self, xdata_loaded, labels_loaded, transform=None):\n",
        "    super().__init__()\n",
        "    self.transform = transform\n",
        "        \n",
        "  def __len__(self):\n",
        "    return xdata_loaded.shape[0]\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return xdata_loaded[idx], labels_loaded[idx]\n",
        "\n",
        "batch_size = 1\n",
        "dataset = ChromosomeDataset(xdata_loaded, labels_loaded)\n",
        "train_ds, test_ds = torch.utils.data.random_split(dataset, (12434, 1000)) # train on 12434 and tested on 1000\n",
        "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "xb, yb = next(iter(train_dataloader))\n",
        "print(xb.shape)\n",
        "print(yb.shape)\n",
        "\n",
        "fig = pl.figure(figsize = (5,5)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(xb.cpu().detach().numpy()[0][0], aspect=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tHO6aehHh79"
      },
      "outputs": [],
      "source": [
        "!rm unet.py\n",
        "!wget https://raw.githubusercontent.com/jvanvugt/pytorch-unet/master/unet.py\n",
        "\n",
        "from unet import UNet\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet(n_classes=4, depth=4, padding=True, up_mode='upsample').to(device)\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(model, input_size=(1,1, 88, 88))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53nBbpqbHjCH"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "learning_rate = 0.00001\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f'---------- epoch: {epoch} ----------')\n",
        "  running_loss = []\n",
        "  IOU_per_epoch = []\n",
        "  accuracy_per_epoch = []\n",
        "\n",
        "  for i, (X, y) in enumerate(train_dataloader):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    X = X.float().to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    y1 = torch.argmax(y, dim=1)\n",
        "    outputs = model(X)\n",
        "    \n",
        "    #print(outputs.shape, y.shape)\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss(outputs, y1)\n",
        "    #loss = Variable(loss, requires_grad = True)\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    a = list(model.parameters())[0].clone()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    b = list(model.parameters())[0].clone()\n",
        "    #print(torch.equal(a.data, b.data))\n",
        "    \n",
        "    #running_loss += loss.item()\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "    outputs1 = torch.argmax(outputs, dim=1)\n",
        "    outputs2 = oneHotEncode(outputs1.cpu().detach().numpy())\n",
        "    \n",
        "    IOU_per_epoch.append(IOU(np.transpose(outputs2, (0,3,1,2)), y.cpu().detach().numpy()))\n",
        "    accuracy_per_epoch.append(globalAccuracy(np.transpose(outputs2, (0,3,1,2)), y.cpu().detach().numpy()))\n",
        "\n",
        "  #print(np.unique(running_loss))\n",
        "  #print(running_loss)\n",
        "  training_loss = sum(running_loss)/len(running_loss)\n",
        "  print(f'train loss: {training_loss}')\n",
        "\n",
        "  accuracy_per_epoch = np.average(np.stack(accuracy_per_epoch, axis=0))\n",
        "  print(f'Accuracy per epoch: {accuracy_per_epoch}')\n",
        "  mean_IOU_per_epoch = np.average(np.stack(IOU_per_epoch, axis=0), axis=0)\n",
        "  print(f'Mean IOU per epoch: {mean_IOU_per_epoch}')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiWmSEupJft8"
      },
      "outputs": [],
      "source": [
        "# check point code\n",
        "PATH = '/content/drive/MyDrive/projects/chromosomes/segmentation_unet/model/unet_chromosome_01.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkEPOMXTJpka"
      },
      "outputs": [],
      "source": [
        "y_pred_test=[]\n",
        "true_y_pred_test=[]\n",
        "i = 0\n",
        "model.eval()\n",
        "for i, (X, y) in enumerate(test_dataloader):\n",
        "  \n",
        "  X = X.float().to(device)\n",
        "  y = y.to(device)\n",
        "\n",
        "  #y1 = torch.argmax(y, dim=1)\n",
        "  outputs = model(X)\n",
        "\n",
        "  true_y_pred_test.append(y.cpu().detach().numpy())\n",
        "  y_pred_test.append(outputs.cpu().detach().numpy())\n",
        "\n",
        "y_pred_test1 = np.stack(y_pred_test, axis=1)[0,:,:,:,:]\n",
        "true_y_pred_test1 = np.stack(true_y_pred_test, axis=1)[0,:,:,:,:]\n",
        "\n",
        "testIOU = IOU(y_pred_test1, true_y_pred_test1)\n",
        "print(f'testIOU: {testIOU}')\n",
        "\n",
        "# Global Accuracy \n",
        "global_test_accuracy = globalAccuracy(np.transpose(oneHotEncode(np.argmax(y_pred_test1, axis=1)), (0,3,1,2)), true_y_pred_test1)\n",
        "print(f'Global Test Acuracy: {global_train_accuracy}')\n",
        "del y_pred_test1\n",
        "del true_y_pred_test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt_LEP8iqj-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Load data\n",
        "# xdata = np.load('xdata_88x88.npy')\n",
        "# labels = np.load('ydata_88x88_0123_onehot.npy')\n",
        "# train_test_boundary_index = round(13434*.8)\n",
        "\n",
        "# model = OverlapSegmentationNet(input_shape=(88,88,1))\n",
        "\n",
        "# # Choose loss\n",
        "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# # Specify the number of epochs to run\n",
        "# num_epoch = 5\n",
        "# for i in range(num_epoch):\n",
        "    \n",
        "#     # Fit\n",
        "#     model.fit(x=xdata, y=labels, epochs=1, validation_split=0.2) \n",
        "#     os.makedirs('models', exist_ok=True)\n",
        "#     filename = 'models/savedmodel_' + str(i) + 'epoch'\n",
        "#     model.save(filename)\n",
        "    \n",
        "#     # Predict and plot images\n",
        "#     predictions = model.predict(xdata[0:4,...])\n",
        "#     utilities.plotSamplesOneHots(predictions[0:4,...].round())\n",
        "   \n",
        "#     # Calculate mIOU\n",
        "#     y_pred_train = model.predict(xdata[0:train_test_boundary_index,...]).round()\n",
        "#     trainIOU = utilities.IOU(y_pred_train, labels[0:train_test_boundary_index,...])\n",
        "#     print('Training IOU: ' + str(trainIOU))\n",
        "#     trainAccuracy = utilities.globalAccuracy(y_pred_train, labels[0:train_test_boundary_index,...])\n",
        "#     print('Training accuracy: ' + str(trainAccuracy))\n",
        "#     del y_pred_train\n",
        "    \n",
        "#     y_pred_test = model.predict(xdata[train_test_boundary_index:,...]).round()\n",
        "#     testIOU = utilities.IOU(y_pred_test, labels[train_test_boundary_index:,...])\n",
        "#     print('Testing IOU: ' + str(testIOU))\n",
        "#     testAccuracy = utilities.globalAccuracy(y_pred_test, labels[train_test_boundary_index:,...])\n",
        "#     print('Testing accuracy: ' + str(testAccuracy))\n",
        "#     del y_pred_test\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOfC0WTEmpzhVvHZofOecM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
